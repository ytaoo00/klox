# Representing Code
This chapter has some wonderful content on design pattern.
## From Lexical Grammar To Syntactic Grammar
Recall that lexical grammar is the rule for how characters get grouped together.<br>
Now we move to syntactic grammar.<br>

| Terminology      | Lexical Grammar | Syntactic Grammar |
|:-----------------|:---------------:|:-----------------:|
| The "alphabet is |    Character    |       Token       |
| A "String" is    | Lexeme or token |    Expression     |
| Implemented By   |     Scanner     |      Parser       |

## From Grammar To String
Recall that in Lexical Grammar, we basically list out all the rules to convert characters to tokens.<br>
We cannot do so in the Syntactic Grammar since Syntactic Grammar contains infinite number of valid strings.<br>
### rules
We can create a finite set of rules that allows creation of infinite strings.

| Terminology  |                     Meaning                      |
|:-------------|:------------------------------------------------:|
| Production   |              Another name for rule               |
| Derivations  |   Strings generated by recursively using rules   |
| Head         |             Name of Production/Rule              |
| Body         |               The generated result               |
| Terminal     |       A Letter from the grammar's alphabet       |
| Nonterminal  | A name reference to another rule in the grammar  |
### examples
````
breakfast -> protein "with" breakfast "on the side" ;
breakfast -> protein ;
breakfast -> bread ;

protein -> "sausage" ;
....

bread -> "toast" ;
````
The convention is ``Head -> Body``.<br>
In the first rule, **breakfast** is the Head(rule_name)<br>
**"with"** and **"on the side"** are Terminal. Here we define terminal must be wrapped with quotes.<br>
**protein**, **breakfast** are Nonterminal. Notice that a rule can refer to the same rule. <br>
Also notice that two rules can have the same name. In which case we can arbitrarily pick one. <br>
Last thing, we define a rule must end with a semicolon. 
### Syntatic sugar
1. pipe ==> |
   - Essentially OR operator
   - ``bread -> "toast" | "biscuits" | "English muffin" ;``
2. Grouping ==> ()
   - ``protein -> ("Scrambled" | "poached" | "fried") "eggs" ;``
3. Start ==> *
   - zero or more times.
   - ``crispiness -> "really" "really"* ;``
4. Plus ==> +
   - one or more times.
   - ``crispiness -> "really" ;``
5. Question mark ==> ?
   - zero or one time, but not more.
   - ``breakfast -> protein ("with" breakfast "on the side") ? ;``

## Lox Expression(For Now)
````
    expression  -> literal | unary | binary | grouping ;
    literal     -> NUMBER | STRING | "true" | "false" | "nul" ;
    grouping    -> "(" expression ")" ;
    unary       -> ( "-" | "!" ) expression ;
    binary      -> expression operator expression ;
    operator    -> "==" | "!=" | "<" | "<=" | ">" | ">=" 
                 | "+"  | "-"  | "*" | "/" ; 
````
Notice:
- we capitalize terminals that are a single lexeme whose text representation may vary.
- This grammar is ambiguous.
  - Sort of jumping ahead
  - Ambiguous means a string that can has more than one parse tree. 
    - EX. 1 + 3 - 3
      - We start from Expression
      - choose binary
      - we can either choose literal -> NUMBER for the first expression
      - or we can choose binary for the first expression
      ````
                                     expression         
                                          |        
                                       binary     
                         /                |             \      
                     expression        operator     expression
                         |                |              |
                     binary              "-"          literal
          /             |             \                  |
      expression     operator     expression             3
          |             |              |
        literal        "+"          literal
          |                            |
          1                            3
      
                       expression         
                            |        
                         binary     
           /                |                \      
        expression       operator        expression
         |                |                 |
      literal            "+"              binary
         |                    /             |             \
         1                expression     operator     expression
                              |             |             |
                           literal         "-"          literal
                              |                           |
                              3                           3
      ````
      - notice that the two parse actually give the same result.
      - But this is not the same case. Ex. 1 * 3 - 3

## Design choice
### Template Code
````java
abstract class Expr {
    static class Binary extends Expr{
        Binary(Expr left, Token operator, Expr right){
            this.left = left;
            this.operator = operator;
            this.right = right;
        }
        final Expr left;
        final Token operator;
        final Expr right;
    }
    //Other expression
}
````
### Domain
What should the AST belongs to? Parser or Interpreter? <br>
Is it parser because it is where the trees are created? If so, then the tree class should have some method for parsing.<br>
Is it interpreter because it is there the trees are consumed? If so, then the tree should have some method for interpreting. <br>
But the truth is, trees span the border between those domains, which means they are owned by neither. <br>
Trees exist to enable the parser and interpreter to communicate, therefore they are designed to be data type with no associated behavior by choice. <br>
Not exactly prefect by Object-Oriented Programming. <br> 

### Boilerplate
The author here did a rather extreme demo of boilerplate. <br>
Because the class declaration for each grammar is somewhat repetitive with little alteration, he wrote a boilerplate which is a code that automatically generate the .java file for each grammar class.<br>
Tools like lombok exists for that purpose. <br>

### Working with tree
````
if (expr instanceof Expr.Binary){
    //....
}else if (expr instanceof Expr.Grouping){
    //....    
}....
````
Intuitively, since each kind of expression behaves differently at runtime, the interpreter needs to select a different chunk of code to handle each expression type. <br>
The most straightforward solution is an if-else list, but it is going to be a long list and it runs slowly. <br>
Because expression type whose names are alphabetically would take longer to execute because they'd fall through more if cases before finding the right type. <br>
#### Solution one
We can put an abstract interpret() method on Expr which each subclass would then implement to interpret itself.<br>
However, we run in to two problems. 
One. As mentioned above, trees span a few domain. Parser also wants to do name resolution, etc. So if we added instance methods to the expression classes for every one of those operations, that would smush different domains together.<br>
That choice violates separation of concerns and leads to hard-to-maintain code. <br>
Two. The expression problem. <br>
Say if we do decide to add instance method, we have the following structure.<br>

| Class       | interpret() | resolve() | analyze() | 
|:------------|:-----------:|:---------:|:---------:|
| Binary      |     ...     |    ...    |    ...    |
| Grouping    |     ...     |    ...    |    ...    |
| Literal     |     ...     |    ...    |    ...    |
| Unary       |     ...     |    ...    |    ...    |
| Others..    |     ...     |    ...    |    ...    |

In OO languages such as Java, methods can be declared rather easily in individual class. <br>
The table can also be extended easily, we just define a new class. No existing code will be touched.<br>
However, if we want to add a new operation (a new column), we have to modify every class in the table. <br>

In Functional language. The table is flipped.<br>
Since types and functions are totally distinct, to implement an operation for a number of different types, one can define a single function.<br>
In the function body, one can use pattern match, sort of type-based switch, to implement the operation for each type all in one pace.<br>
It sort of like our intuitive approach. <br>
````
 interper()      analyze()     
____________   ____________
| Binary   |   | Binary   |
| Grouping |   | Grouping |
|......... |   |......... |
````
In that case, adding a new type is hard. One have to go adding a new case to all the pattern match functions. <br>
#### Solution Two ---> The Visitor Pattern
The visitor pattern is about approximating the functional style within an OOP language.<br>
We can define all the behavior for a new operation on a set of types in one place, without having to touch the type themselves.<br>
It does so by adding a layer of indirection.<br>
The author did a wonderful example using Pastry, Beignet(French deep-fried donuts), and Cruller(US deep-fried donuts).<br>
I did recently visit New Orleans and the Beignet is wonderful.<br>

````java
abstract class Pastry{
    
}

class Beignet extends Pastry{
    
}

class Cruller extends Pastry{
    
}
````
A similar scenario will be if we want to define new pastry operations, cooking, eating, decorating, etc. without having to add a new method to each class every time.<br>
First we define a separate interface.<br>
````java
interface PastryVisitor{
    void visitBeignet(Beignet beignet);
    void visitCruller(Cruller cruller);
}
````
Each operation that can be performed on pastries is a new class that implements that interface. <br>
It has a concrete method for each type of pastry which keeps the code for the operation on both types live in one class.<br>
For example:
````java
class Chef implements PastryVisitor{
    @Override
    void visitBeignet(Beignet beignet){
        //....
    }
    
    @Override
    void visitCruller(Cruller cruller){
        //....
    }
}
````
How to route to the correct method on the visitor based on its type? Polymorphism.<br>
Polymorphism is when you can treat an object as a generic version of something, but when you access it, the code determines which exact type it is and calls the associated code.<br>
We add an accept method to the abstract class and each class that implements it. 
````java
abstract class Pastry{
    abstract void accept(PastryVisitor visitor);
}

class Beignet extends Pastry{
    @Override
    void accept(PastryVisitor visitor){
        //again polymorphism
        visitor.visitBeignet(this);
    }
    
}

class Cruller extends Pastry{
    @Override
    void accept(PastryVisitor visitor){
        visitor.visitCruller(this);
    }
}
````
For a chef to prepare a Pastry
````java
class Chef implements PastryVisitor{
    @Override
    void visitBeignet(Beignet beignet){
        //....
        preparePastry(nextOrder);
    }
    
    @Override
    void visitCruller(Cruller cruller){
        //....
        preparePastry(nextOrder);
    }
    
    void preparePastry(Pastry pastry){
        //polymorphism in play
        //will called the specific subclass;s overriding implementation of accept
        pastry.accept(this);
    }
}
````
In general, to perform an operation on a pastry, we call its accept() method and pass in the visitor for the operation we want to execute.<br>
accept(this) -> takes in an instance of chef, which is a instance of visitor. <br>
Due to polymorphism, pastry.accept(this) will trigger the specific subclass's accept, whether that be (Beignet) or (Cruller)<br>
Then in the specific subclass, say Beignet, the accept method will in turn calls the appropriate visit method.<br>
This way, we have one accept() method to each class, but we can have as many visitors as we want without ever having to touch the pastry classes again.<br>
A bit twisty, but very clever.<br>

